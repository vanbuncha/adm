{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks\n",
    "1. Data Retrieval and Preprocessing\n",
    "- Obtain MovieLens 1M dataset. []\n",
    "- Load dataset. []\n",
    "- Check data integrity: []\n",
    "- Address issues like missing movies. []\n",
    "- Handle data inconsistencies (e.g., user IDs with additional data, ratings for non-existent movies). []\n",
    "- Create User-Item Interaction Matrix. []\n",
    "- Split data for 5-fold cross-validation. []\n",
    "- Handling Cold Starts (dealing with users or items not seen during training). []\n",
    "2. Recommendation Algorithms\n",
    "- Implement Naive Approaches: []\n",
    "- Global Average Rating. []\n",
    "- Average Rating per Item. []\n",
    "- Average Rating per User. []\n",
    "- Optimal Linear Combination with and without bias. []\n",
    "- Implement UV Matrix Decomposition. []\n",
    "- Implement Matrix Factorization with Gradient Descent and Regularization. []\n",
    "- For each algorithm, calculate: \n",
    "- RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error). []\n",
    "- Address Cold Starts for the implemented algorithms. []\n",
    "3. Visualization\n",
    "- Apply dimensionality reduction techniques for visualization: []\n",
    "- PCA (Principal Component Analysis). []\n",
    "- t-SNE (t-Distributed Stochastic Neighbor Embedding). []\n",
    "- UMAP (Uniform Manifold Approximation and Projection). []\n",
    "4. Documentation and Reporting \n",
    "- Document code, algorithms, and preprocessing steps. []\n",
    "- Summarize and analyze the results of each algorithm. []\n",
    "- Provide insights into the best-performing algorithms. []\n",
    "- Discuss challenges and limitations encountered during the implementation. []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data sets\n",
    "\n",
    "# movies\n",
    "df_movies = pd.read_csv(\"ml-1m/movies.dat\", sep='::', encoding='ISO-8859-1', header=None, engine='python', names=['MovieID', 'Title', 'Genres'])\n",
    "df_movies = df_movies.rename({0: 'MovieID', 1: 'Title', 2: 'Genre'}, axis='columns')\n",
    "\n",
    "#ratings\n",
    "df_ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep='::', encoding='ISO-8859-1', header=None, engine='python', names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
    "\n",
    "#users\n",
    "df_users = pd.read_csv(\"ml-1m/users.dat\", sep='::', encoding='ISO-8859-1', header=None, engine=\"python\", names=['UserID', 'Gender', 'Age', 'Occupation', 'ZipCode'])\n",
    "df_users.columns = ['UserID', 'Gender', 'Age', 'Occupation', 'ZipCode']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID        0\n",
       "Gender        0\n",
       "Age           0\n",
       "Occupation    0\n",
       "ZipCode       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in ratings dataset\n",
    "df_ratings.isnull().sum()\n",
    "\n",
    "# Check for missing values in movies dataset\n",
    "df_movies.isnull().sum()\n",
    "\n",
    "# Check for missing values in users dataset\n",
    "df_users.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "   UserID Gender  Age  Occupation ZipCode\n",
      "0       1      F    1          10   48067\n",
      "1       2      M   56          16   70072\n",
      "2       3      M   25          15   55117\n",
      "3       4      M   45           7   02460\n",
      "4       5      M   25          20   55455\n"
     ]
    }
   ],
   "source": [
    "# Checking data integrity\n",
    "\n",
    "# movies\n",
    "print(df_movies.head())\n",
    "print\n",
    "\n",
    "# ratings\n",
    "print(df_ratings.head())\n",
    "print\n",
    "\n",
    "# users\n",
    "\n",
    "print(df_users.head())\n",
    "\n",
    "# In movies.dat there is missing movieID of 91 we create a placeholder\n",
    "new_movie = pd.DataFrame({'MovieID': [91], 'Title': ['Unknown'], 'Genres': ['Unknown']})\n",
    "df_movies = pd.concat([df_movies, new_movie], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train Users: 4832, Test Users: 1208\n",
      "Fold 2 - Train Users: 4832, Test Users: 1208\n",
      "Fold 3 - Train Users: 4832, Test Users: 1208\n",
      "Fold 4 - Train Users: 4832, Test Users: 1208\n",
      "Fold 5 - Train Users: 4832, Test Users: 1208\n"
     ]
    }
   ],
   "source": [
    "# Users split into 5\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Kfold object to split data into\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert the DataFrame to a list to ensure consistent indices\n",
    "user_data = df_users.values\n",
    "ratings_data = df_ratings.values\n",
    "\n",
    "# Split the data into 5 folds\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(user_data)):\n",
    "    # Split the users dataset\n",
    "    train_users = df_users.iloc[train_indices]\n",
    "    test_users = df_users.iloc[test_indices]\n",
    "\n",
    "    # Split the ratings dataset\n",
    "    train_ratings = df_ratings[df_ratings['UserID'].isin(train_users['UserID'])]\n",
    "    test_ratings = df_ratings[df_ratings['UserID'].isin(test_users['UserID'])]\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Train Users: {len(train_users)}, Test Users: {len(test_users)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train Movies: 3107, Test Movies: 777\n",
      "Fold 2 - Train Movies: 3107, Test Movies: 777\n",
      "Fold 3 - Train Movies: 3107, Test Movies: 777\n",
      "Fold 4 - Train Movies: 3107, Test Movies: 777\n",
      "Fold 5 - Train Movies: 3108, Test Movies: 776\n"
     ]
    }
   ],
   "source": [
    "# Movie split into 5\n",
    "num_folds = 5\n",
    "\n",
    "# Kfold object to split data into\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert the DataFrame to a list to ensure consistent indices\n",
    "movie_data = df_movies.values\n",
    "\n",
    "# Split the data into 5 folds\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(movie_data)):\n",
    "    # Split the movies dataset\n",
    "    train_movies = df_movies.iloc[train_indices]\n",
    "    test_movies = df_movies.iloc[test_indices]\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Train Movies: {len(train_movies)}, Test Movies: {len(test_movies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train Ratings: 800167, Test Ratings: 200042\n",
      "Fold 2 - Train Ratings: 800167, Test Ratings: 200042\n",
      "Fold 3 - Train Ratings: 800167, Test Ratings: 200042\n",
      "Fold 4 - Train Ratings: 800167, Test Ratings: 200042\n",
      "Fold 5 - Train Ratings: 800168, Test Ratings: 200041\n"
     ]
    }
   ],
   "source": [
    "# Ratins into 5\n",
    "num_folds = 5\n",
    "\n",
    "# Kfold object to split data into\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert the DataFrame to a list to ensure consistent indices\n",
    "ratings_data = df_ratings.values\n",
    "\n",
    "# Split the data into 5 folds\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(ratings_data)):\n",
    "    # Split the ratings dataset\n",
    "    train_ratings = df_ratings.iloc[train_indices]\n",
    "    test_ratings = df_ratings.iloc[test_indices]\n",
    "\n",
    "    print(f\"Fold {fold + 1} - Train Ratings: {len(train_ratings)}, Test Ratings: {len(test_ratings)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "   UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "2       3      M   25          15    55117\n",
      "3       4      M   45           7    02460\n",
      "4       5      M   25          20    55455\n",
      "   MovieID                               Title                         Genre\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "recommenderSystem.Naive_1() missing 1 required positional argument: 'test_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X15sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mprint\u001b[39m(df_movies\u001b[39m.\u001b[39mhead())\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X15sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m rec\u001b[39m=\u001b[39m recommenderSystem()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X15sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m df_filled\u001b[39m=\u001b[39mrec\u001b[39m.\u001b[39;49mNaive_1(df_ratings)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X15sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m rec\u001b[39m.\u001b[39mmatrix_factorization(df_filled)\n",
      "\u001b[0;31mTypeError\u001b[0m: recommenderSystem.Naive_1() missing 1 required positional argument: 'test_df'"
     ]
    }
   ],
   "source": [
    "class recommenderSystem():\n",
    "\n",
    "\n",
    "    def Naive_1(self, train_df, test_df):\n",
    "        # Naive Approach\n",
    "        r_item = train_df.groupby('MovieID')['Rating'].mean().reset_index().rename({'Rating':\n",
    "                                                                        'R_item'},axis='columns')\n",
    "        \n",
    "        r_user = train_df.groupby('UserID')['Rating'].mean().reset_index().rename({'Rating':\n",
    "                                                                        'R_user'},axis='columns')\n",
    "\n",
    "        train_df=train_df.merge(r_item, on=['MovieID']).merge(r_user, on=['UserID'])\n",
    "\n",
    "        #Handle instances were we do not have instances in the training set of movies/users in test set\n",
    "        test_df=test_df.merge(r_item, on=['MovieID']).merge(r_user, on=['UserID'])\n",
    "        test_only_users = set(test_df['user_id']) - set(train_df['user_id'])\n",
    "        test_only_movies = set(test_df['movie_id']) - set(train_df['movie_id'])\n",
    "        global_average_rating = train_df['Rating'].mean()\n",
    "        for user in test_only_users:\n",
    "            test_df.loc[test_df['user_id'] == user, 'R_user'] = global_average_rating\n",
    "\n",
    "        for movie in test_only_movies:\n",
    "            test_df.loc[test_df['movie_id'] == movie, 'R_item'] = global_average_rating\n",
    "\n",
    "\n",
    "        X = train_df[['R_item','R_user']]\n",
    "        y = train_df['Rating']\n",
    "        model = LinearRegression().fit(X, y)\n",
    "\n",
    "        alpha, beta = model.coef_\n",
    "        gamma = model.intercept_\n",
    "\n",
    "        X_test = test_df[['R_item','R_user']]\n",
    "        y_test = test_df[\"Rating\"]\n",
    "        \n",
    "        # Predict ratings for the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate the root mean squared error (RMSE) for the predictions\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        # Print the coefficients and RMSE\n",
    "        print(f\"Alpha: {alpha}, Beta: {beta}, Gamma: {gamma}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "\n",
    "    def matrix_factorization(self, train_df, test_df):\n",
    "        # The Matrix Factorization\n",
    "\n",
    "        num_factors=10\n",
    "        num_iter=75\n",
    "        reg=0.05\n",
    "        lr=0.005\n",
    "        num_users = df_ratings['UserID'].nunique()\n",
    "        num_movies = df_ratings['MovieID'].nunique()\n",
    "        \n",
    "        U = np.random.rand(num_users, num_factors)\n",
    "        V = np.random.rand(num_movies, num_factors).T\n",
    "        R = pd.pivot_table(train_df, index='UserID', columns='MovieID', values='Rating').values\n",
    "        for i in range(num_iter):\n",
    "            for i in range(num_users):\n",
    "                for j in range(num_movies):\n",
    "                    error = R[i][j] -  np.dot(U[i, :], V[:, j])\n",
    "                    for k in range(num_factors):\n",
    "                        U[i][k] += lr * (error * V[k][j] - reg*  U[i][k])\n",
    "                        V[k][j] += lr * (error * U[i][k] - reg*  V[k][j])\n",
    "            # Compute loss\n",
    "            total_error = 0\n",
    "            for i in range(num_users):\n",
    "                for j in range(num_movies):\n",
    "                    if R[i][j] > 0:\n",
    "                        total_error += (R[i][j] - np.dot(U[i, :], V[:, j]))**2\n",
    "\n",
    "            print(f\"Total Training Error epoch {e}: {total_error}\")\n",
    "       # Test the model on the test set and compute the RMSE\n",
    "            test_matrix = pd.pivot_table(test_df, index='UserID', columns='MovieID', values='Rating').values\n",
    "            test_error = 0\n",
    "            count = 0\n",
    "            \n",
    "            for i in range(num_users):\n",
    "                for j in range(num_movies):\n",
    "                    if not np.isnan(test_matrix[i][j]):\n",
    "                        pred = np.dot(U[i, :], V[:, j])\n",
    "                        test_error += (test_matrix[i][j] - pred)**2\n",
    "                        count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                test_rmse = sqrt(test_error / count)\n",
    "                print(f\"Iteration {iteration + 1}: Test RMSE = {test_rmse}\")\n",
    "        \n",
    "    def visualisation_1(self):\n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(data)\n",
    "        \n",
    "    def visualisation_2(self):\n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, verbose=1)\n",
    "        tsne_result = tsne.fit_transform(data)\n",
    "        \n",
    "    def visualisation_3(self):\n",
    "        # Apply UMAP\n",
    "        umap_model = umap.UMAP(n_components=2)\n",
    "        umap_result = umap_model.fit_transform(data)\n",
    "\n",
    "    def cross_validation(self,df_ratings, folds, model):\n",
    "        # prepare cross validation\n",
    "        # Shuffle DataFrame\n",
    "        df_ratings = df_ratings.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Split DataFrame into folds\n",
    "        num_rows = len(df_ratings)\n",
    "        fold_size = num_rows // folds\n",
    "        splits = []\n",
    "\n",
    "        for i in range(folds):\n",
    "            start_index = i * fold_size\n",
    "            end_index = (i + 1) * fold_size if i < folds - 1 else num_rows\n",
    "            test_df = df_ratings.iloc[start_index:end_index]\n",
    "            train_df = pd.concat([df_ratings.iloc[:start_index], df_ratings.iloc[end_index:]])\n",
    "            if (model==\"Naive\"):\n",
    "                train_df, test_df=self.Naive_1(train_df, test_df)\n",
    "            elif (model==\"Matrix\"):\n",
    "                train_df, test_df = self.matrix_factorization(train_df, test_df)\n",
    "    \n",
    "    def perf_measures(y_true,y_pred):\n",
    "        # Calculate RMSE (Root Mean Squared Error)\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        print(f'RMSE: {rmse}')\n",
    "\n",
    "        # Calculate MAE (Mean Absolute Error)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        print(f'MAE: {mae}')\n",
    "        \n",
    "    def main():\n",
    "        train_list,test_list=self.cross_validartion(5);\n",
    "        \n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "            # Specify the file path\n",
    "        file_path = 'ml-1m/ratings.dat'\n",
    "        df_ratings = pd.read_csv(file_path, sep='::',header=None, engine='python')\n",
    "        df_ratings = df_ratings.rename({0: 'UserID',\n",
    "                                        1:'MovieID',\n",
    "                                        2:'Rating',\n",
    "                                        3:'Timestamp'},axis='columns')\n",
    "\n",
    "        print(df_ratings.head())\n",
    "        # Specify the file path\n",
    "        file_path = 'ml-1m/users.dat'\n",
    "        df_users = pd.read_csv(file_path, sep='::',header=None, engine='python')\n",
    "        df_users = df_users.rename({0: 'UserID',\n",
    "                                        1:'Gender',\n",
    "                                        2:'Age',\n",
    "                                        3:'Occupation',\n",
    "                                        4: 'Zip-code'\n",
    "                                        },axis='columns')\n",
    "        print(df_users.head())\n",
    "        # Specify the file path\n",
    "        file_path = 'ml-1m/movies.dat'\n",
    "        df_movies = pd.read_csv(file_path, sep='::', header=None, encoding='ISO-8859-1', engine='python')\n",
    "        df_movies = df_movies.rename({0: 'MovieID',\n",
    "                                        1:'Title',\n",
    "                                        2:'Genre'},axis='columns')\n",
    "        \n",
    "\n",
    "        \n",
    "        print(df_movies.head())\n",
    "        rec= recommenderSystem()\n",
    "        df_filled=rec.Naive_1(df_ratings)\n",
    "        \n",
    "        rec.matrix_factorization(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The UV matrix decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Interaction Matrix:\n",
      "[[ 5. nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 3. nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# Load the ratings data\n",
    "file_path_ratings = 'ml-1m/ratings.dat'\n",
    "df_ratings = pd.read_csv(file_path_ratings, sep='::', header=None, engine='python')\n",
    "df_ratings = df_ratings.rename(columns={0: 'UserID', 1: 'MovieID', 2: 'Rating', 3: 'Timestamp'})\n",
    "\n",
    "# Load the movies data\n",
    "file_path_movies = 'ml-1m/movies.dat'\n",
    "df_movies = pd.read_csv(file_path_movies, sep='::', header=None, engine='python', encoding='latin-1')\n",
    "df_movies = df_movies.rename(columns={0: 'MovieID', 1: 'Title', 2: 'Genres'})\n",
    "\n",
    "# Merge the ratings and movies data based on MovieID\n",
    "df_merged = df_ratings.merge(df_movies, on='MovieID')\n",
    "\n",
    "# Create the user-item interaction matrix\n",
    "user_item_matrix = df_merged.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "\n",
    "# Optionally, convert the Pandas DataFrame to a NumPy array\n",
    "user_item_matrix = user_item_matrix.values\n",
    "\n",
    "# Display the user-item interaction matrix\n",
    "print(\"User-Item Interaction Matrix:\")\n",
    "print(user_item_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of U: (6040, 10)\n",
      "Shape of V: (3706, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the ratings data\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python')\n",
    "\n",
    "# Create the user-item interaction matrix\n",
    "user_item_matrix = ratings.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "\n",
    "# Data preparation: Filter out rows/columns with missing values (NaN)\n",
    "valid_indices = ~np.isnan(user_item_matrix)\n",
    "user_item_matrix = user_item_matrix[valid_indices]\n",
    "\n",
    "# Calculate the global average rating\n",
    "global_average = user_item_matrix.mean().mean()\n",
    "\n",
    "# Initialize the U and V matrices with the global average divided by the number of latent factors\n",
    "num_factors = 10  # Number of latent factors\n",
    "num_users, num_items = user_item_matrix.shape  # Get the number of users and items from the filtered user-item matrix\n",
    "U = np.full((num_users, num_factors), global_average / num_factors)\n",
    "V = np.full((num_items, num_factors), global_average / num_factors)\n",
    "\n",
    "# Display the shapes of U and V\n",
    "print(\"Shape of U:\", U.shape)\n",
    "print(\"Shape of V:\", V.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, RMSE: 1.4150\n",
      "Shape of U after training: (6040, 10)\n",
      "Shape of V after training: (3706, 10)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01  \n",
    "num_epochs = 1  \n",
    "regularization = 0.1  # Regularization term\n",
    "\n",
    "def calculate_rmse(user_item_matrix, U, V):\n",
    "    predicted_ratings = np.dot(U, V.T)\n",
    "    observed_ratings = user_item_matrix\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(np.nanmean((predicted_ratings - observed_ratings) ** 2))\n",
    "    return rmse\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for user in range(num_users):\n",
    "        for item in range(num_items):\n",
    "            if not np.isnan(user_item_matrix.iloc[user, item]):\n",
    "                # Calculate the error (difference between actual and predicted rating)\n",
    "                error = user_item_matrix.iloc[user, item] - np.dot(U[user, :], V[item, :])\n",
    "                \n",
    "                # Update U and V using gradient descent\n",
    "                U[user, :] += learning_rate * (error * V[item, :] - regularization * U[user, :])\n",
    "                V[item, :] += learning_rate * (error * U[user, :] - regularization * V[item, :])\n",
    "    \n",
    "    # Calculate RMSE at the end of each epoch\n",
    "    rmse = calculate_rmse(user_item_matrix, U, V)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Display the shapes of U and V after training \n",
    "print(\"Shape of U after training:\", U.shape)\n",
    "print(\"Shape of V after training:\", V.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, RMSE: 0.9315\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store RMSE values for each epoch\n",
    "rmse_values = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for user in range(num_users):\n",
    "        for item in range(num_items):\n",
    "            if not np.isnan(user_item_matrix.iloc[user, item]):\n",
    "                # Calculate the error (difference between actual and predicted rating)\n",
    "                error = user_item_matrix.iloc[user, item] - np.dot(U[user, :], V[item, :])\n",
    "                \n",
    "                # Update U and V using gradient descent\n",
    "                U[user, :] += learning_rate * (error * V[item, :] - regularization * U[user, :])\n",
    "                V[item, :] += learning_rate * (error * U[user, :] - regularization * V[item, :])\n",
    "    \n",
    "    # Calculate RMSE at the end of each epoch\n",
    "    predicted_ratings = np.dot(U, V.T)\n",
    "    rmse = np.sqrt(np.nanmean((user_item_matrix - predicted_ratings) ** 2))\n",
    "    rmse_values.append(rmse)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Specify the number of splits (K) for cross-validation\n",
    "K = 5\n",
    "\n",
    "# Initialize KFold with the number of splits (K)\n",
    "kf = KFold(n_splits=K)\n",
    "\n",
    "# Convert user_item_matrix to a NumPy array (if not already)\n",
    "user_item_matrix = user_item_matrix.values\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_indices, val_indices in kf.split(user_item_matrix):\n",
    "    train_indices = np.array(train_indices)  # Convert indices to NumPy array\n",
    "    val_indices = np.array(val_indices)  # Convert indices to NumPy array\n",
    "\n",
    "    # Select valid rows from the user_item_matrix using the indices\n",
    "    train_set = user_item_matrix[train_indices, :]\n",
    "    val_set = user_item_matrix[val_indices, :]\n",
    "\n",
    "    # Continue with training and evaluation steps for each fold\n",
    "    # You can perform UV matrix decomposition, train the model, and evaluate it on the validation set\n",
    "    # ...\n",
    "\n",
    "    # After each fold, you can store the evaluation results (e.g., RMSE or MAE) for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Loop over the folds\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_indices, val_indices \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(user_item_matrix):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Select valid rows from the user_item_matrix using the indices\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_set \u001b[39m=\u001b[39m user_item_matrix[train_indices, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     val_set \u001b[39m=\u001b[39m user_item_matrix[val_indices, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanbuncha/devenv/leidenuniv/adm/as1/recom_sys.ipynb#X46sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Initialize U and V matrices with random values\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the ratings data\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', names=['UserID', 'MovieID', 'Rating', 'Timestamp'], engine='python')\n",
    "\n",
    "# Create the user-item interaction matrix\n",
    "user_item_matrix = ratings.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "user_item_matrix = user_item_matrix.values\n",
    "\n",
    "# Data preparation: Filter out rows/columns with missing values (NaN)\n",
    "valid_indices = ~np.isnan(user_item_matrix)\n",
    "user_item_matrix = user_item_matrix[valid_indices]\n",
    "\n",
    "# Define the number of latent factors\n",
    "num_factors = 10\n",
    "\n",
    "# Create a KFold cross-validator with 5 folds\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Initialize a list to store RMSE values for each fold\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop over the folds\n",
    "for train_indices, val_indices in kf.split(user_item_matrix):\n",
    "    # Select valid rows from the user_item_matrix using the indices\n",
    "    train_set = user_item_matrix[train_indices, :]\n",
    "    val_set = user_item_matrix[val_indices, :]\n",
    "\n",
    "    # Initialize U and V matrices with random values\n",
    "    num_users, num_items = train_set.shape\n",
    "    U = np.random.rand(num_users, num_factors)\n",
    "    V = np.random.rand(num_items, num_factors)\n",
    "\n",
    "    # Set hyperparameters for training\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for user in range(num_users):\n",
    "            for item in range(num_items):\n",
    "                if not np.isnan(train_set[user, item]):\n",
    "                    error = train_set[user, item] - np.dot(U[user, :], V[item, :])\n",
    "                    U[user, :] += learning_rate * (error * V[item, :])\n",
    "                    V[item, :] += learning_rate * (error * U[user, :])\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_indices = np.where(~np.isnan(val_set))\n",
    "    val_predictions = np.dot(U[train_indices, :], V.T[:, val_indices[1]])\n",
    "\n",
    "    # Calculate RMSE for the current fold\n",
    "    rmse = np.sqrt(mean_squared_error(val_set[val_indices], val_predictions))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Calculate the average RMSE over all folds\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "# Print the average RMSE\n",
    "print(\"Average RMSE:\", avg_rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as1_adm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
